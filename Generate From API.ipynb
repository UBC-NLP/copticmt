{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeSCR6KPFa09"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ImwPvBQ_7Ba"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(101)\n",
    "\n",
    "df_test_dialect_msa = pd.DataFrame(columns=[\"ex_id\",'Dialect_Original', \"GPT4_MSA_pred\",\t\"Gemini_MSA_pred\",\t\"ClaudeOpus_MSA_pred\",\t\"Llama70B_MSA_pred\"])\n",
    "\n",
    "# add data to Dialect_Original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YluJCI5qFhLs"
   },
   "source": [
    "# Claude Opus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7KkVpvCDivM",
    "outputId": "3838055a-b1fb-43a6-8bdb-4755905f2b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.26.1-py3-none-any.whl (877 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jiter<1,>=0.1.0 (from anthropic)\n",
      "  Downloading jiter-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
      "Installing collected packages: jiter, h11, httpcore, httpx, anthropic\n",
      "Successfully installed anthropic-0.26.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.4.0\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlXsb1mEGhPK"
   },
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=\"[API_KEY]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0k063hJvJHe"
   },
   "outputs": [],
   "source": [
    "source_language = \"Coptic\"\n",
    "target_language = \"Arabic\"\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7H1dRBPvJaQ",
    "outputId": "2c3dc122-b91e-4211-f61e-ed44e5ea89ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:25,  4.41s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, raw in tqdm(df_test_dialect_msa[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  zero_shot_example = f\"\"\"### Translate this sentence from {source_language} to {target_language} (generate only the {target_language} translation), Source:\n",
    "  {raw[f\"{source_language}_Original\"]}\n",
    "  ### Target:\n",
    "  \"\"\"\n",
    "  try:\n",
    "    message = client.messages.create(\n",
    "      model=\"claude-3-opus-20240229\",\n",
    "      max_tokens=1000,\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": zero_shot_example\n",
    "                  }\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "    )\n",
    "    df_test_dialect_msa.loc[index, f\"ClaudeOpus_{target_language}_pred\"] = message.content[0].text\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    message = client.messages.create(\n",
    "      model=\"claude-3-opus-20240229\",\n",
    "      max_tokens=1000,\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": zero_shot_example\n",
    "                  }\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "    )\n",
    "    df_test_dialect_msa.loc[index, f\"ClaudeOpus_{target_language}_pred\"] = message.content[0].text\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"Coptic\"\n",
    "target_language = \"Arabic\"\n",
    "counter = 0\n",
    "\n",
    "for index, raw in tqdm(test100_5shots_arabic[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  selected = test100_5shots_arabic.iloc[int(index)]\n",
    "\n",
    "  source_examples = []\n",
    "  target_examples = []\n",
    "\n",
    "  for i in range(5):\n",
    "    source_examples.append(selected.loc[f'{source_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "    target_examples.append(selected.loc[f'{target_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "\n",
    "  source_sentence = selected.loc[f'{source_language}_Original'].replace(\"\\n\",\"\")\n",
    "  target_sentence = selected.loc[f'{target_language}_Original'].replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "  five_shot_example = f\"\"\"Translate this from {source_language} into 1. {target_language} (generate only the {target_language} for number 6):\n",
    "  {source_examples[0]}\n",
    "  1. {target_examples[0]}\n",
    "  Translate this from {source_language} into 2. {target_language}:\n",
    "  {source_examples[1]}\n",
    "  2. {target_examples[1]}\n",
    "  Translate this from {source_language} into 3. {target_language}:\n",
    "  {source_examples[2]}\n",
    "  3. {target_examples[2]}\n",
    "  Translate this from {source_language} into 4. {target_language}:\n",
    "  {source_examples[3]}\n",
    "  4. {target_examples[3]}\n",
    "  Translate this from {source_language} into 5. {target_language}:\n",
    "  {source_examples[4]}\n",
    "  5. {target_examples[4]}\n",
    "  Translate this from {source_language} into 6. {target_language}:\n",
    "  {source_sentence}\n",
    "  6.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    message = client.messages.create(\n",
    "      model=\"claude-3-opus-20240229\",\n",
    "      max_tokens=1000,\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": five_shot_example\n",
    "                  }\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "    )\n",
    "    # print(message.content[0].text.replace(\"6.\", \"\"))\n",
    "    # break\n",
    "    test100_5shots_arabic.loc[index, f\"ClaudeOpus_{target_language}_pred\"] = message.content[0].text.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    message = client.messages.create(\n",
    "      model=\"claude-3-opus-20240229\",\n",
    "      max_tokens=1000,\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\n",
    "                      \"type\": \"text\",\n",
    "                      \"text\": five_shot_example\n",
    "                  }\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "    )\n",
    "    test100_5shots_arabic.loc[index, f\"ClaudeOpus_{target_language}_pred\"] = message.content[0].text.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwRB03V_AmSy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTEqs8QwOBJ2"
   },
   "source": [
    "#gemini pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzMDpDfayNqY"
   },
   "outputs": [],
   "source": [
    "%pip3 install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY1TNFr3ySdN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWJ5D7MTySia"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRVYKZtIySlb"
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MvQ2b-Eypmr"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-1.5-pro-preview-0514\"  # @param {type:\"string\"}\n",
    "\n",
    "model = GenerativeModel(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qj21Rrk3yppJ",
    "outputId": "ab593816-0700-4c2c-b9bc-eff989ddfed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens: 14\n",
      "total_billable_characters: 29\n",
      "\n",
      "\n",
      "Answer:\n",
      "It's great that you like bagels! They are delicious and versatile. \n",
      "\n",
      "Is there anything else you'd like to tell me about your love for bagels? For example:\n",
      "\n",
      "* What kind of bagels are your favorite? \n",
      "* What do you like to put on your bagels? \n",
      "* Where do you get your bagels from? \n",
      "\n",
      "Let's talk bagels! üòä \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a example model with system instructions\n",
    "example_model = GenerativeModel(\n",
    "    MODEL_ID,\n",
    "    # system_instruction=[\n",
    "    #     \"You are a helpful language translator.\",\n",
    "    #     \"Your mission is to translate text in English to French.\",\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# Set model parameters\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=8192,\n",
    ")\n",
    "\n",
    "# Set safety settings\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}\n",
    "\n",
    "# safe = [\n",
    "#     {\n",
    "#         \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "#         \"threshold\": \"BLOCK_NONE\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "#         \"threshold\": \"BLOCK_NONE\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "#         \"threshold\": \"BLOCK_NONE\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "#         \"threshold\": \"BLOCK_NONE\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Set contents to send to the model\n",
    "contents = [prompt]\n",
    "\n",
    "# Counts tokens\n",
    "print(example_model.count_tokens(contents))\n",
    "\n",
    "# Prompt the model to generate content\n",
    "response = example_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")\n",
    "\n",
    "# Print the model response\n",
    "print(f\"\\nAnswer:\\n{response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b2hFtuPPpOD"
   },
   "outputs": [],
   "source": [
    "source_language = \"Dialect\"\n",
    "target_language = \"MSA\"\n",
    "counter = 0\n",
    "finished_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBTZpHU7GWvN"
   },
   "outputs": [],
   "source": [
    "for index, raw in tqdm(df_test_dialect_msa[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  zero_shot_example = f\"\"\"### Translate this sentence from {source_language} to {target_language} (generate only the {target_language} translation), Source:\n",
    "  {raw[f\"{source_language}_Original\"]}\n",
    "  ### Target:\n",
    "  \"\"\"\n",
    "  try:\n",
    "    contents = [zero_shot_example]\n",
    "    response = example_model.generate_content(\n",
    "        contents,\n",
    "        generation_config=generation_config,\n",
    "        # safety_settings=safety_settings,\n",
    "    )\n",
    "    if str(response.candidates[0].finish_reason) == \"FinishReason.STOP\":\n",
    "      df_test_dialect_msa.loc[index, f\"Gemini_{target_language}_pred\"] = response.text\n",
    "      counter += 1\n",
    "      finished_counter += 1\n",
    "    else:\n",
    "      df_test_dialect_msa.loc[index, f\"Gemini_{target_language}_pred\"] = \"No Answer\"\n",
    "      counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 50 seconds\")\n",
    "    time.sleep(100)\n",
    "    contents = [zero_shot_example]\n",
    "    response = example_model.generate_content(\n",
    "        contents,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    # print(response.text)\n",
    "    df_test_dialect_msa.loc[index, f\"Gemini_{target_language}_pred\"] = response.text\n",
    "    counter += 1\n",
    "    # response = client.chat.completions.create(\n",
    "    #   model=\"gpt-4-turbo-2024-04-09\",\n",
    "    #   messages=[\n",
    "    #     {\n",
    "    #       \"role\": \"user\",\n",
    "    #       \"content\": \"\"\n",
    "    #     }\n",
    "    #   ],\n",
    "    #   temperature=1,\n",
    "    #   max_tokens=256,\n",
    "    #   top_p=1,\n",
    "    #   frequency_penalty=0,\n",
    "    #   presence_penalty=0\n",
    "    # )\n",
    "    # dev100_english.loc[index, f\"Gemini_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    # counter += 1\n",
    "\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cdkt0LSzBFIb"
   },
   "outputs": [],
   "source": [
    "source_language = \"Arabic\"\n",
    "target_language = \"English\"\n",
    "counter = 0\n",
    "\n",
    "for index, raw in tqdm(test100_5shots_arabic_english[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  selected = test100_5shots_arabic_english.iloc[int(index)]\n",
    "\n",
    "  source_examples = []\n",
    "  target_examples = []\n",
    "\n",
    "  for i in range(5):\n",
    "    source_examples.append(selected.loc[f'{source_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "    target_examples.append(selected.loc[f'{target_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "\n",
    "  source_sentence = selected.loc[f'{source_language}_Original'].replace(\"\\n\",\"\")\n",
    "  target_sentence = selected.loc[f'{target_language}_Original'].replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "  five_shot_example = f\"\"\"Translate this from {source_language} into 1. {target_language} (generate only {target_language} translation for number 6):\n",
    "  {source_examples[0]}\n",
    "  1. {target_examples[0]}\n",
    "  Translate this from {source_language} into 2. {target_language}:\n",
    "  {source_examples[1]}\n",
    "  2. {target_examples[1]}\n",
    "  Translate this from {source_language} into 3. {target_language}:\n",
    "  {source_examples[2]}\n",
    "  3. {target_examples[2]}\n",
    "  Translate this from {source_language} into 4. {target_language}:\n",
    "  {source_examples[3]}\n",
    "  4. {target_examples[3]}\n",
    "  Translate this from {source_language} into 5. {target_language}:\n",
    "  {source_examples[4]}\n",
    "  5. {target_examples[4]}\n",
    "  Translate this from {source_language} into 6. {target_language}:\n",
    "  {source_sentence}\n",
    "  6.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    contents = [five_shot_example]\n",
    "    response = example_model.generate_content(\n",
    "        contents,\n",
    "        generation_config=generation_config,\n",
    "        # safety_settings=safety_settings,\n",
    "    )\n",
    "    if str(response.candidates[0].finish_reason) == \"FinishReason.STOP\":\n",
    "      test100_5shots_arabic_english.loc[index, f\"Gemini_{target_language}_pred\"] = response.text\n",
    "      counter += 1\n",
    "      finished_counter += 1\n",
    "    else:\n",
    "      test100_5shots_arabic_english.loc[index, f\"Gemini_{target_language}_pred\"] = \"No Answer\"\n",
    "      counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 50 seconds\")\n",
    "    time.sleep(100)\n",
    "    while True:\n",
    "      try:\n",
    "              contents = [five_shot_example]\n",
    "              response = example_model.generate_content(\n",
    "                  contents,\n",
    "                  generation_config=generation_config,\n",
    "                  safety_settings=safety_settings,\n",
    "              )\n",
    "              # print(response.text)\n",
    "              test100_5shots_arabic_english.loc[index, f\"Gemini_{target_language}_pred\"] = response.text\n",
    "              counter += 1\n",
    "      except ValueError:\n",
    "          print(f\"overloading error at {counter}, sleeping for 50 seconds\")\n",
    "      else:\n",
    "          break\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlP0CDBqBICQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeHM0J9iBIEP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B93Pwtp4iFT"
   },
   "source": [
    "# GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4bACnxC4jwP",
    "outputId": "df844482-08d1-4ab7-bdc6-e7c06aedb866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/320.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m317.4/320.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.30.1\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l5hWTOm4luW"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH0t6kmd92c0"
   },
   "outputs": [],
   "source": [
    "source_language = \"Dialect\"\n",
    "target_language = \"MSA\"\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rh8XTWPl92c0",
    "outputId": "aa1d3cc1-2770-4e17-c01f-8f1b1afce0fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [05:37,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, raw in tqdm(df_test_dialect_msa[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  zero_shot_example = f\"\"\"### Translate this sentence from {source_language} to {target_language} (generate only the {target_language} translation), Source:\n",
    "  {raw[f\"{source_language}_Original\"]}\n",
    "  ### Target:\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": zero_shot_example\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "  # print(response.choices[0].message.content)\n",
    "    df_test_dialect_msa.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": zero_shot_example\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "  # print(response.choices[0].message.content)\n",
    "    df_test_dialect_msa.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    counter += 1\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ755cwjBYlU"
   },
   "outputs": [],
   "source": [
    "source_language = \"Coptic\"\n",
    "target_language = \"Arabic\"\n",
    "counter = 0\n",
    "\n",
    "for index, raw in tqdm(test100_5shots_arabic[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  selected = test100_5shots_arabic.iloc[int(index)]\n",
    "\n",
    "  source_examples = []\n",
    "  target_examples = []\n",
    "\n",
    "  for i in range(5):\n",
    "    source_examples.append(selected.loc[f'{source_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "    target_examples.append(selected.loc[f'{target_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "\n",
    "  source_sentence = selected.loc[f'{source_language}_Original'].replace(\"\\n\",\"\")\n",
    "  target_sentence = selected.loc[f'{target_language}_Original'].replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "  five_shot_example = f\"\"\"Translate this from {source_language} into 1. {target_language} (generate only the {target_language} for number 6):\n",
    "  {source_examples[0]}\n",
    "  1. {target_examples[0]}\n",
    "  Translate this from {source_language} into 2. {target_language}:\n",
    "  {source_examples[1]}\n",
    "  2. {target_examples[1]}\n",
    "  Translate this from {source_language} into 3. {target_language}:\n",
    "  {source_examples[2]}\n",
    "  3. {target_examples[2]}\n",
    "  Translate this from {source_language} into 4. {target_language}:\n",
    "  {source_examples[3]}\n",
    "  4. {target_examples[3]}\n",
    "  Translate this from {source_language} into 5. {target_language}:\n",
    "  {source_examples[4]}\n",
    "  5. {target_examples[4]}\n",
    "  Translate this from {source_language} into 6. {target_language}:\n",
    "  {source_sentence}\n",
    "  6.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": five_shot_example\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "    # break\n",
    "    # dev100_english.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    test100_5shots_arabic.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo-2024-04-09\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": five_shot_example\n",
    "        }\n",
    "      ],\n",
    "      temperature=1,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "    # break\n",
    "    # dev100_english.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    test100_5shots_arabic.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZJXR_4h9f5K"
   },
   "source": [
    "# llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWBliIQL9jyU",
    "outputId": "41f7b1fa-67f8-4ac5-d1db-7fea7e4b62bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.5.0-py3-none-any.whl (75 kB)\n",
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.18.2)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YynSInXV9sBA",
    "outputId": "06c893ac-9e3e-4078-c24c-cbe894053463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's AI landscape, and their importance can be summarized as follows:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of natural language inputs, which is essential for applications that require immediate responses, such as:\n",
      "\t* Conversational AI, chatbots, and virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "\t* Sentiment analysis and opinion mining in social media monitoring\n",
      "\t* Language translation and interpretation in multilingual settings\n",
      "2. **Low Latency**: Fast language models minimize latency, which is critical in applications where timely responses are vital, such as:\n",
      "\t* Real-time sentiment analysis for stock market predictions\n",
      "\t* Language-based game development (e.g., language-based puzzles, word games)\n",
      "\t* Real-time language translation in video conferencing or online meetings\n",
      "3. **Scalability**: Fast language models enable the processing of large volumes of text data, making them suitable for applications that involve:\n",
      "\t* Large-scale text classification and categorization\n",
      "\t* Information retrieval and search engines\n",
      "\t* Natural Language Processing (NLP) for big data analytics\n",
      "4. **Efficient Resource Utilization**: Fast language models optimize computational resources, reducing the need for:\n",
      "\t* High-performance computing hardware\n",
      "\t* Extensive memory allocation\n",
      "\t* Energy consumption, contributing to environmentally friendly AI\n",
      "5. **Improved User Experience**: Fast language models enable more responsive and interactive interfaces, leading to:\n",
      "\t* Enhanced user engagement and satisfaction\n",
      "\t* Increased adoption and usage of language-based applications\n",
      "\t* Better overall user experience in applications like voice assistants, chatbots, and language translation apps\n",
      "6. **Competitive Advantage**: Organizations that can process and analyze large amounts of text data quickly can:\n",
      "\t* Gain insights faster than competitors\n",
      "\t* React more swiftly to market trends and customer needs\n",
      "\t* Develop more effective marketing strategies and customer engagement initiatives\n",
      "7. **Edge AI and IoT Applications**: Fast language models are essential for Edge AI applications, where data processing occurs at the edge of the network, reducing latency and improving real-time decision-making in:\n",
      "\t* Autonomous vehicles\n",
      "\t* Smart home devices\n",
      "\t* Industrial automation and IoT systems\n",
      "8. **Research and Development**: Fast language models accelerate research in NLP, enabling scientists to explore new ideas and applications more quickly, such as:\n",
      "\t* Language generation and text synthesis\n",
      "\t* Multimodal language processing (e.g., speech, vision, and text)\n",
      "\t* Explanable AI and transparency in language models\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications, from real-time processing and low latency to scalability, efficient resource utilization, and improved user experience. They also provide a competitive advantage, enable Edge AI and IoT applications, and accelerate research and development in NLP.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key= \"\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY-o4TWeA7Iw"
   },
   "outputs": [],
   "source": [
    "source_language = \"Dialect\"\n",
    "target_language = \"MSA\"\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibkcz6VrA7Ix",
    "outputId": "f0cc3617-28c0-4667-a3de-0563b564aafe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [03:21,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, raw in tqdm(df_test_dialect_msa[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  zero_shot_example = f\"\"\"### Translate this sentence from {source_language} to {target_language} (generate only the {target_language} translation), Source:\n",
    "  {raw[f\"{source_language}_Original\"]}\n",
    "  ### Target:\n",
    "  \"\"\"\n",
    "  try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": zero_shot_example,\n",
    "              }\n",
    "          ],\n",
    "          model=\"llama3-70b-8192\",\n",
    "      )\n",
    "  # print(chat_completion.choices[0].message.content)\n",
    "    df_test_dialect_msa.loc[index, f\"Llama70B_{target_language}_pred\"] = chat_completion.choices[0].message.content\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": zero_shot_example,\n",
    "              }\n",
    "          ],\n",
    "          model=\"llama3-70b-8192\",\n",
    "      )\n",
    "  # print(chat_completion.choices[0].message.content)\n",
    "    df_test_dialect_msa.loc[index, f\"Llama70B_{target_language}_pred\"] = chat_completion.choices[0].message.content\n",
    "    counter += 1\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"Coptic\"\n",
    "target_language = \"Arabic\"\n",
    "counter = 0\n",
    "\n",
    "for index, raw in tqdm(test100_5shots_arabic[counter:].iterrows()):\n",
    "  # print(raw['English_Original'])\n",
    "  # print(raw['Coptic_Original'])\n",
    "  selected = test100_5shots_arabic.iloc[int(index)]\n",
    "\n",
    "  source_examples = []\n",
    "  target_examples = []\n",
    "\n",
    "  for i in range(5):\n",
    "    source_examples.append(selected.loc[f'{source_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "    target_examples.append(selected.loc[f'{target_language}_shot_{i}'].replace(\"\\n\",\"\"))\n",
    "\n",
    "  source_sentence = selected.loc[f'{source_language}_Original'].replace(\"\\n\",\"\")\n",
    "  target_sentence = selected.loc[f'{target_language}_Original'].replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "  five_shot_example = f\"\"\"Translate this from {source_language} into 1. {target_language} (generate only the {target_language} for number 6):\n",
    "  {source_examples[0]}\n",
    "  1. {target_examples[0]}\n",
    "  Translate this from {source_language} into 2. {target_language}:\n",
    "  {source_examples[1]}\n",
    "  2. {target_examples[1]}\n",
    "  Translate this from {source_language} into 3. {target_language}:\n",
    "  {source_examples[2]}\n",
    "  3. {target_examples[2]}\n",
    "  Translate this from {source_language} into 4. {target_language}:\n",
    "  {source_examples[3]}\n",
    "  4. {target_examples[3]}\n",
    "  Translate this from {source_language} into 5. {target_language}:\n",
    "  {source_examples[4]}\n",
    "  5. {target_examples[4]}\n",
    "  Translate this from {source_language} into 6. {target_language}:\n",
    "  {source_sentence}\n",
    "  6.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": five_shot_example,\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "    # break\n",
    "    # dev100_english.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    test100_5shots_arabic.loc[index, f\"Llama70B_{target_language}_pred\"] = chat_completion.choices[0].message.content.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "  except:\n",
    "    print(f\"overloading error at {counter}, sleeping for 30 seconds\")\n",
    "    time.sleep(30)\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": five_shot_example,\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "    # break\n",
    "    # dev100_english.loc[index, f\"GPT4_{target_language}_pred\"] = response.choices[0].message.content\n",
    "    test100_5shots_arabic.loc[index, f\"Llama70B_{target_language}_pred\"] = chat_completion.choices[0].message.content.replace(\"6.\", \"\")\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "  # print(zero_shot_example)\n",
    "  # print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YluJCI5qFhLs",
    "ZTEqs8QwOBJ2",
    "3B93Pwtp4iFT",
    "TZJXR_4h9f5K"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
